{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOs5YzdIPlwJdrc4O8kUzRy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LayanAlrashoud/Space-Categorization/blob/main/Space.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. تحميل البيانات من Kaggle\n",
        "path = kagglehub.dataset_download(\"abhikalpsrivastava15/space-images-category\")\n",
        "\n",
        "# طباعة المسار إلى الملفات\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# 2. المسار إلى مجلد \"space images\"\n",
        "space_images_path = os.path.join(path, \"space images\")\n",
        "\n",
        "# التحقق من المجلد الرئيسي\n",
        "if os.path.exists(space_images_path):\n",
        "    categories = [folder_name for folder_name in os.listdir(space_images_path)\n",
        "                  if os.path.isdir(os.path.join(space_images_path, folder_name)) and not folder_name in [\"train\", \"val\", \"test\"]]\n",
        "    print(\"Actual folder names in 'space images':\", categories)\n",
        "else:\n",
        "    print(\"'space images' folder does not exist. Please check the dataset structure.\")\n",
        "\n",
        "# 3. إنشاء مسارات `train`, `val`, `test`\n",
        "output_dirs = {\n",
        "    \"train\": os.path.join(path, \"train\"),\n",
        "    \"val\": os.path.join(path, \"val\"),\n",
        "    \"test\": os.path.join(path, \"test\"),\n",
        "}\n",
        "\n",
        "for output_dir in output_dirs.values():\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "# 4. تقسيم الصور بين `train`, `val`, `test`\n",
        "def split_data(category_path, category_name, output_dirs):\n",
        "    # جمع الصور\n",
        "    valid_extensions = ('.png', '.jpg', '.jpeg')\n",
        "    images = [os.path.join(category_path, img) for img in os.listdir(category_path) if img.endswith(valid_extensions)]\n",
        "\n",
        "    # تحقق من وجود الصور\n",
        "    if len(images) == 0:\n",
        "        print(f\"No valid images found in {category_path}\")\n",
        "        return\n",
        "\n",
        "    # تقسيم الصور\n",
        "    train_images, test_images = train_test_split(images, test_size=0.3, random_state=42)\n",
        "    val_images, test_images = train_test_split(test_images, test_size=0.33, random_state=42)\n",
        "\n",
        "    # نسخ الصور إلى المجلدات المناسبة\n",
        "    for img in train_images:\n",
        "        shutil.copy(img, os.path.join(output_dirs[\"train\"], category_name))\n",
        "    for img in val_images:\n",
        "        shutil.copy(img, os.path.join(output_dirs[\"val\"], category_name))\n",
        "    for img in test_images:\n",
        "        shutil.copy(img, os.path.join(output_dirs[\"test\"], category_name))\n",
        "\n",
        "# 5. تنفيذ التقسيم\n",
        "for category in categories:\n",
        "    category_path = os.path.join(space_images_path, category)\n",
        "    category_cleaned = category.replace(\" - Google Search\", \"\")\n",
        "    for output_dir in output_dirs.values():\n",
        "        category_folder = os.path.join(output_dir, category_cleaned)\n",
        "        if not os.path.exists(category_folder):\n",
        "            os.makedirs(category_folder)\n",
        "    split_data(category_path, category_cleaned, output_dirs)\n",
        "\n",
        "print(\"Data has been split into train, val, and test folders successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCoY-y7pDnY0",
        "outputId": "4e1a99ed-b520-4db8-cc98-cd9fc649cc89"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/abhikalpsrivastava15/space-images-category?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 464M/464M [00:07<00:00, 65.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/abhikalpsrivastava15/space-images-category/versions/1\n",
            "Actual folder names in 'space images': ['constellation - Google Search', 'nebula - Google Search', 'planets - Google Search', 'stars - Google Search', 'galaxies - Google Search', 'cosmos space - Google Search']\n",
            "Data has been split into train, val, and test folders successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# إعداد تحسين البيانات\n",
        "data_gen_args = {\n",
        "    \"rescale\": 1.0 / 255,\n",
        "    \"rotation_range\": 20,\n",
        "    \"width_shift_range\": 0.1,\n",
        "    \"height_shift_range\": 0.1,\n",
        "    \"zoom_range\": 0.1,\n",
        "    \"horizontal_flip\": True,\n",
        "    \"fill_mode\": \"nearest\",\n",
        "}\n",
        "\n",
        "train_datagen = ImageDataGenerator(**data_gen_args)\n",
        "val_test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    output_dirs[\"train\"],\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    output_dirs[\"val\"],\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    output_dirs[\"test\"],\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "# تحميل VGG16\n",
        "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(150, 150, 3))\n",
        "base_model.trainable = True\n",
        "\n",
        "# إلغاء تجميد آخر 8 طبقات فقط\n",
        "for layer in base_model.layers[:-6]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# بناء النموذج\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(train_generator.class_indices), activation='softmax')\n",
        "])\n",
        "\n",
        "# تجميع النموذج\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# إعداد الكولباك\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7)\n",
        "\n",
        "# تدريب النموذج\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=30,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# تقييم النموذج\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# حفظ النموذج\n",
        "model.save(\"space_classification_model_vgg16_improved.keras\")\n",
        "print(\"Model has been saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fmxwXRtmgfd",
        "outputId": "5aa70b82-21d9-42cf-98d1-b9f2f476d618"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 773 images belonging to 6 classes.\n",
            "Found 221 images belonging to 6 classes.\n",
            "Found 113 images belonging to 6 classes.\n",
            "Epoch 1/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 14s/step - accuracy: 0.1743 - loss: 1.8792 - val_accuracy: 0.4072 - val_loss: 1.6359 - learning_rate: 1.0000e-05\n",
            "Epoch 2/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 14s/step - accuracy: 0.2623 - loss: 1.7135 - val_accuracy: 0.5113 - val_loss: 1.4808 - learning_rate: 1.0000e-05\n",
            "Epoch 3/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 14s/step - accuracy: 0.3539 - loss: 1.5882 - val_accuracy: 0.5339 - val_loss: 1.3244 - learning_rate: 1.0000e-05\n",
            "Epoch 4/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 15s/step - accuracy: 0.4282 - loss: 1.4316 - val_accuracy: 0.5430 - val_loss: 1.2181 - learning_rate: 1.0000e-05\n",
            "Epoch 5/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 14s/step - accuracy: 0.5134 - loss: 1.3304 - val_accuracy: 0.5792 - val_loss: 1.1488 - learning_rate: 1.0000e-05\n",
            "Epoch 6/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 14s/step - accuracy: 0.4958 - loss: 1.3025 - val_accuracy: 0.5973 - val_loss: 1.1214 - learning_rate: 1.0000e-05\n",
            "Epoch 7/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 14s/step - accuracy: 0.5647 - loss: 1.2112 - val_accuracy: 0.5973 - val_loss: 1.0977 - learning_rate: 1.0000e-05\n",
            "Epoch 8/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 14s/step - accuracy: 0.5547 - loss: 1.1781 - val_accuracy: 0.6244 - val_loss: 1.0586 - learning_rate: 1.0000e-05\n",
            "Epoch 9/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 14s/step - accuracy: 0.5990 - loss: 1.1010 - val_accuracy: 0.5837 - val_loss: 1.0705 - learning_rate: 1.0000e-05\n",
            "Epoch 10/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 14s/step - accuracy: 0.6093 - loss: 1.0208 - val_accuracy: 0.5882 - val_loss: 1.0542 - learning_rate: 1.0000e-05\n",
            "Epoch 11/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 14s/step - accuracy: 0.5975 - loss: 1.0205 - val_accuracy: 0.6244 - val_loss: 1.0163 - learning_rate: 1.0000e-05\n",
            "Epoch 12/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 14s/step - accuracy: 0.6398 - loss: 0.9848 - val_accuracy: 0.6063 - val_loss: 1.0110 - learning_rate: 1.0000e-05\n",
            "Epoch 13/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 14s/step - accuracy: 0.6361 - loss: 1.0112 - val_accuracy: 0.6109 - val_loss: 0.9952 - learning_rate: 1.0000e-05\n",
            "Epoch 14/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 14s/step - accuracy: 0.6398 - loss: 0.9873 - val_accuracy: 0.6199 - val_loss: 1.0053 - learning_rate: 1.0000e-05\n",
            "Epoch 15/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 15s/step - accuracy: 0.6558 - loss: 0.9714 - val_accuracy: 0.6154 - val_loss: 0.9924 - learning_rate: 1.0000e-05\n",
            "Epoch 16/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 14s/step - accuracy: 0.6627 - loss: 0.8841 - val_accuracy: 0.6290 - val_loss: 0.9743 - learning_rate: 1.0000e-05\n",
            "Epoch 17/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 14s/step - accuracy: 0.6927 - loss: 0.8719 - val_accuracy: 0.6380 - val_loss: 0.9663 - learning_rate: 1.0000e-05\n",
            "Epoch 18/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 14s/step - accuracy: 0.6689 - loss: 0.8783 - val_accuracy: 0.6154 - val_loss: 0.9626 - learning_rate: 1.0000e-05\n",
            "Epoch 19/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 15s/step - accuracy: 0.6643 - loss: 0.8653 - val_accuracy: 0.6290 - val_loss: 0.9454 - learning_rate: 1.0000e-05\n",
            "Epoch 20/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 15s/step - accuracy: 0.6523 - loss: 0.8961 - val_accuracy: 0.6199 - val_loss: 0.9591 - learning_rate: 1.0000e-05\n",
            "Epoch 21/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 14s/step - accuracy: 0.7308 - loss: 0.7825 - val_accuracy: 0.6561 - val_loss: 0.9456 - learning_rate: 1.0000e-05\n",
            "Epoch 22/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 14s/step - accuracy: 0.6812 - loss: 0.8241 - val_accuracy: 0.5973 - val_loss: 0.9723 - learning_rate: 1.0000e-05\n",
            "Epoch 23/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 14s/step - accuracy: 0.7123 - loss: 0.7635 - val_accuracy: 0.6561 - val_loss: 0.9321 - learning_rate: 2.0000e-06\n",
            "Epoch 24/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 15s/step - accuracy: 0.7050 - loss: 0.7813 - val_accuracy: 0.6425 - val_loss: 0.9240 - learning_rate: 2.0000e-06\n",
            "Epoch 25/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 14s/step - accuracy: 0.7446 - loss: 0.7379 - val_accuracy: 0.6471 - val_loss: 0.9305 - learning_rate: 2.0000e-06\n",
            "Epoch 26/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 15s/step - accuracy: 0.7139 - loss: 0.7410 - val_accuracy: 0.6290 - val_loss: 0.9334 - learning_rate: 2.0000e-06\n",
            "Epoch 27/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 14s/step - accuracy: 0.7021 - loss: 0.7744 - val_accuracy: 0.6290 - val_loss: 0.9310 - learning_rate: 2.0000e-06\n",
            "Epoch 28/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 15s/step - accuracy: 0.7217 - loss: 0.7599 - val_accuracy: 0.6425 - val_loss: 0.9293 - learning_rate: 4.0000e-07\n",
            "Epoch 29/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 14s/step - accuracy: 0.7351 - loss: 0.7226 - val_accuracy: 0.6516 - val_loss: 0.9262 - learning_rate: 4.0000e-07\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 7s/step - accuracy: 0.6707 - loss: 1.0566\n",
            "Test Loss: 1.044641375541687\n",
            "Test Accuracy: 0.6637167930603027\n",
            "Model has been saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KPJ7Om1Lnet0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}